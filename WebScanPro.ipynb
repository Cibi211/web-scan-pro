{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c46ad7c7-5e99-49a3-b304-33d99115a64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\cibis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfcc27db-2b41-4128-96e0-d4dbac4159d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling:   0%|                                                                                 | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawled: http://localhost:8080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling:  20%|██████████████▌                                                          | 2/10 [00:02<00:08,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawled: http://localhost:8080/instructions.php\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling:  30%|█████████████████████▉                                                   | 3/10 [00:04<00:10,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawled: http://localhost:8080/setup.php\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling:  40%|█████████████████████████████▏                                           | 4/10 [00:06<00:10,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawled: http://localhost:8080/vulnerabilities/brute\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling:  50%|████████████████████████████████████▌                                    | 5/10 [00:08<00:09,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawled: http://localhost:8080/vulnerabilities/exec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling:  60%|███████████████████████████████████████████▊                             | 6/10 [00:10<00:07,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawled: http://localhost:8080/vulnerabilities/csrf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling:  70%|███████████████████████████████████████████████████                      | 7/10 [00:12<00:05,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawled: http://localhost:8080/vulnerabilities/fi/?page=include.php\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling:  80%|██████████████████████████████████████████████████████████▍              | 8/10 [00:14<00:03,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawled: http://localhost:8080/vulnerabilities/upload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling:  90%|█████████████████████████████████████████████████████████████████▋       | 9/10 [00:16<00:01,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawled: http://localhost:8080/vulnerabilities/captcha\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling: 100%|████████████████████████████████████████████████████████████████████████| 10/10 [00:18<00:00,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Crawled: http://localhost:8080/vulnerabilities/sqli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling: 100%|████████████████████████████████████████████████████████████████████████| 10/10 [00:20<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 SQLi detected in form http://localhost:8080/vulnerabilities/brute field=username payload=1'--\n",
      "🔥 SQLi detected in form http://localhost:8080/vulnerabilities/brute field=username payload=' UNION SELECT NULL--\n",
      "🔥 SQLi detected in form http://localhost:8080/vulnerabilities/sqli field=id payload=1'--\n",
      "🔥 SQLi detected in form http://localhost:8080/vulnerabilities/sqli field=id payload=' UNION SELECT NULL--\n",
      "\n",
      "=== Vulnerability Findings ===\n",
      "[SQLi-FORM]\n",
      "   URL: http://localhost:8080/vulnerabilities/brute\n",
      "   Field: username\n",
      "   Payload: 1'--\n",
      "   Error: you have an error in your sql syntax;\n",
      "--------------------------------------------------\n",
      "[SQLi-FORM]\n",
      "   URL: http://localhost:8080/vulnerabilities/brute\n",
      "   Field: username\n",
      "   Payload: ' UNION SELECT NULL--\n",
      "   Error: you have an error in your sql syntax;\n",
      "--------------------------------------------------\n",
      "[SQLi-FORM]\n",
      "   URL: http://localhost:8080/vulnerabilities/sqli\n",
      "   Field: id\n",
      "   Payload: 1'--\n",
      "   Error: you have an error in your sql syntax;\n",
      "--------------------------------------------------\n",
      "[SQLi-FORM]\n",
      "   URL: http://localhost:8080/vulnerabilities/sqli\n",
      "   Field: id\n",
      "   Payload: ' UNION SELECT NULL--\n",
      "   Error: you have an error in your sql syntax;\n",
      "--------------------------------------------------\n",
      "\n",
      "📂 Results saved to scan_results.csv\n",
      "\n",
      "=== Mitigation Tips ===\n",
      "SQLi Fixes: Use parameterized queries (PreparedStatements), ORM, input validation.\n",
      "XSS Fixes: Validate input, encode output, apply Content Security Policy (CSP).\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import csv\n",
    "from urllib.parse import urljoin, urldefrag, urlparse, parse_qs, urlencode, urlunparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==============================\n",
    "# SQL Injection Payloads & Errors (Safe List)\n",
    "# ==============================\n",
    "SQL_PAYLOADS = [\n",
    "    \"' OR '1'='1\",\n",
    "    \"\\\" OR \\\"1\\\"=\\\"1\",\n",
    "    \"1'--\",\n",
    "    \"1 OR 1=1\",\n",
    "    \"' UNION SELECT NULL--\"\n",
    "    # ❌ Removed dangerous payloads like DROP TABLE\n",
    "]\n",
    "\n",
    "SQL_ERROR_PATTERNS = [\n",
    "    \"you have an error in your sql syntax;\",\n",
    "    \"warning: mysql\",\n",
    "    \"unclosed quotation mark after the character string\",\n",
    "    \"quoted string not properly terminated\",\n",
    "    \"pg_query\",\n",
    "    \"mysql_fetch\",\n",
    "    \"sql error\"\n",
    "]\n",
    "\n",
    "def find_sql_errors(html):\n",
    "    \"\"\"Scan response HTML for common SQL error messages.\"\"\"\n",
    "    for pattern in SQL_ERROR_PATTERNS:\n",
    "        if pattern.lower() in html.lower():\n",
    "            return True, pattern\n",
    "    return False, None\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# XSS Payloads & Reflection Patterns\n",
    "# ==============================\n",
    "XSS_PAYLOADS = [\n",
    "    \"<script>alert(1)</script>\",\n",
    "    \"<img src=x onerror=alert(1)>\",\n",
    "    \"<svg onload=alert(1)>\"\n",
    "]\n",
    "\n",
    "REFLECT_PATTERNS = [re.compile(re.escape(p), re.IGNORECASE) for p in XSS_PAYLOADS]\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# DVWA Web Crawler\n",
    "# ==============================\n",
    "class DVWACrawler:\n",
    "    def __init__(self, base_url, username=\"admin\", password=\"password\",\n",
    "                 max_pages=20, delay=2):  # ⬅ Delay increased to 2s\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.max_pages = max_pages\n",
    "        self.delay = delay\n",
    "        self.session = requests.Session()\n",
    "        self.visited = set()\n",
    "        self.queue = deque([self.base_url])\n",
    "        self.pages = {}\n",
    "        self.forms = {}\n",
    "\n",
    "        # login first\n",
    "        self.login(username, password)\n",
    "\n",
    "    def normalize_url(self, url):\n",
    "        \"\"\"Normalize URL by stripping fragments/trailing slash.\"\"\"\n",
    "        clean_url = urldefrag(url)[0].rstrip(\"/\")\n",
    "        return clean_url\n",
    "\n",
    "    def login(self, username, password):\n",
    "        \"\"\"Logs into DVWA using session-based authentication.\"\"\"\n",
    "        login_url = f\"{self.base_url}/login.php\"\n",
    "        resp = self.session.get(login_url)\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        token = soup.find(\"input\", {\"name\": \"user_token\"})\n",
    "        token_val = token[\"value\"] if token else \"\"\n",
    "\n",
    "        payload = {\n",
    "            \"username\": username,\n",
    "            \"password\": password,\n",
    "            \"Login\": \"Login\",\n",
    "            \"user_token\": token_val\n",
    "        }\n",
    "\n",
    "        login_resp = self.session.post(login_url, data=payload)\n",
    "        if \"Login failed\" in login_resp.text:\n",
    "            raise Exception(\"Login failed: check credentials or CSRF token.\")\n",
    "        print(\"✅ Login successful\")\n",
    "\n",
    "    def extract_links(self, html, page_url):\n",
    "        \"\"\"Extract all same-domain links from a page.\"\"\"\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = []\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            abs_url = urljoin(page_url, a[\"href\"])\n",
    "            clean_url = self.normalize_url(abs_url)\n",
    "            if clean_url.startswith(self.base_url):\n",
    "                links.append(clean_url)\n",
    "        return links\n",
    "\n",
    "    def extract_forms(self, html, page_url):\n",
    "        \"\"\"Extract form details (method, action, inputs).\"\"\"\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        forms = []\n",
    "        for form in soup.find_all(\"form\"):\n",
    "            details = {\n",
    "                \"method\": form.get(\"method\", \"get\").lower(),\n",
    "                \"action\": urljoin(page_url, form.get(\"action\", \"\")),\n",
    "                \"inputs\": []\n",
    "            }\n",
    "            for inp in form.find_all([\"input\", \"textarea\", \"select\"]):\n",
    "                details[\"inputs\"].append({\n",
    "                    \"name\": inp.get(\"name\"),\n",
    "                    \"type\": inp.get(\"type\", inp.name),\n",
    "                    \"value\": inp.get(\"value\", \"\")\n",
    "                })\n",
    "            forms.append(details)\n",
    "        return forms\n",
    "\n",
    "    def crawl(self):\n",
    "        \"\"\"Main crawl loop.\"\"\"\n",
    "        with tqdm(total=self.max_pages, desc=\"Crawling\") as pbar:\n",
    "            while self.queue and len(self.visited) < self.max_pages:\n",
    "                url = self.normalize_url(self.queue.popleft())\n",
    "\n",
    "                if url in self.visited:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    resp = self.session.get(url, timeout=10)\n",
    "                    resp.raise_for_status()\n",
    "                    html = resp.text\n",
    "                except Exception as e:\n",
    "                    print(f\" Error fetching {url}: {e}\")\n",
    "                    self.visited.add(url)\n",
    "                    continue\n",
    "\n",
    "                # store page\n",
    "                self.pages[url] = html\n",
    "\n",
    "                # extract & store forms\n",
    "                page_forms = self.extract_forms(html, url)\n",
    "                if page_forms:\n",
    "                    self.forms[url] = page_forms\n",
    "\n",
    "                # extract & enqueue links\n",
    "                for link in self.extract_links(html, url):\n",
    "                    if link not in self.visited and link not in self.queue:\n",
    "                        self.queue.append(link)\n",
    "\n",
    "                self.visited.add(url)\n",
    "                print(f\"✅ Crawled: {url}\")\n",
    "                pbar.update(1)\n",
    "\n",
    "                time.sleep(self.delay)\n",
    "\n",
    "        return {\"pages\": self.pages, \"forms\": self.forms, \"session\": self.session}\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# SQL Injection Scanner\n",
    "# ==============================\n",
    "class SQLiScanner:\n",
    "    def __init__(self, session, timeout=10):\n",
    "        self.session = session\n",
    "        self.timeout = timeout\n",
    "        self.findings = []\n",
    "\n",
    "    def test_url_params(self, url):\n",
    "        \"\"\"Inject SQL payloads into URL query parameters.\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        params = parse_qs(parsed.query)\n",
    "\n",
    "        if not params:\n",
    "            return\n",
    "\n",
    "        for param in params:\n",
    "            for payload in SQL_PAYLOADS:\n",
    "                test_params = params.copy()\n",
    "                test_params[param] = payload\n",
    "                new_query = urlencode(test_params, doseq=True)\n",
    "                new_url = urlunparse(parsed._replace(query=new_query))\n",
    "\n",
    "                try:\n",
    "                    resp = self.session.get(new_url, timeout=self.timeout)\n",
    "                    found, pattern = find_sql_errors(resp.text)\n",
    "                    if found:\n",
    "                        self.findings.append({\n",
    "                            \"type\": \"SQLi-URL\",\n",
    "                            \"url\": new_url,\n",
    "                            \"parameter\": param,\n",
    "                            \"payload\": payload,\n",
    "                            \"error\": pattern\n",
    "                        })\n",
    "                        print(f\"🔥 SQLi detected in {new_url} param={param} payload={payload}\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "                time.sleep(0.2)  # ⬅ slow down\n",
    "\n",
    "    def test_forms(self, forms_by_url):\n",
    "        \"\"\"Inject SQL payloads into HTML forms.\"\"\"\n",
    "        for page_url, forms in forms_by_url.items():\n",
    "            for form in forms:\n",
    "                action = form.get(\"action\") or page_url\n",
    "                method = form.get(\"method\", \"get\").lower()\n",
    "\n",
    "                base_data = {inp[\"name\"]: (inp.get(\"value\") or \"test\")\n",
    "                             for inp in form[\"inputs\"] if inp.get(\"name\")}\n",
    "\n",
    "                for field in base_data:\n",
    "                    for payload in SQL_PAYLOADS:\n",
    "                        test_data = base_data.copy()\n",
    "                        test_data[field] = payload\n",
    "\n",
    "                        try:\n",
    "                            if method == \"post\":\n",
    "                                resp = self.session.post(action, data=test_data,\n",
    "                                                         timeout=self.timeout)\n",
    "                            else:\n",
    "                                resp = self.session.get(action, params=test_data,\n",
    "                                                        timeout=self.timeout)\n",
    "\n",
    "                            found, pattern = find_sql_errors(resp.text)\n",
    "                            if found:\n",
    "                                self.findings.append({\n",
    "                                    \"type\": \"SQLi-FORM\",\n",
    "                                    \"url\": action,\n",
    "                                    \"field\": field,\n",
    "                                    \"payload\": payload,\n",
    "                                    \"error\": pattern\n",
    "                                })\n",
    "                                print(f\"🔥 SQLi detected in form {action} field={field} payload={payload}\")\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                        time.sleep(0.2)  # ⬅ slow down\n",
    "\n",
    "    def run(self, pages, forms):\n",
    "        \"\"\"Run scanner against crawler results.\"\"\"\n",
    "        for url in pages.keys():\n",
    "            self.test_url_params(url)\n",
    "        self.test_forms(forms)\n",
    "        return self.findings\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# XSS Scanner\n",
    "# ==============================\n",
    "class XSSScanner:\n",
    "    def __init__(self, session, timeout=10):\n",
    "        self.session = session\n",
    "        self.timeout = timeout\n",
    "        self.findings = []\n",
    "\n",
    "    def test_url_params(self, url):\n",
    "        \"\"\"Inject XSS payloads into URL query parameters.\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        params = parse_qs(parsed.query)\n",
    "\n",
    "        if not params:\n",
    "            return\n",
    "\n",
    "        for param in params:\n",
    "            for payload in XSS_PAYLOADS:\n",
    "                test_params = params.copy()\n",
    "                test_params[param] = payload\n",
    "                new_query = urlencode(test_params, doseq=True)\n",
    "                new_url = urlunparse(parsed._replace(query=new_query))\n",
    "\n",
    "                try:\n",
    "                    resp = self.session.get(new_url, timeout=self.timeout)\n",
    "                    for pattern in REFLECT_PATTERNS:\n",
    "                        if pattern.search(resp.text):\n",
    "                            self.findings.append({\n",
    "                                \"type\": \"XSS-URL\",\n",
    "                                \"url\": new_url,\n",
    "                                \"parameter\": param,\n",
    "                                \"payload\": payload\n",
    "                            })\n",
    "                            print(f\"⚡ XSS detected in {new_url} param={param} payload={payload}\")\n",
    "                            break\n",
    "                except Exception:\n",
    "                    pass\n",
    "                time.sleep(0.2)\n",
    "\n",
    "    def test_forms(self, forms_by_url):\n",
    "        \"\"\"Inject XSS payloads into HTML forms.\"\"\"\n",
    "        for page_url, forms in forms_by_url.items():\n",
    "            for form in forms:\n",
    "                action = form.get(\"action\") or page_url\n",
    "                method = form.get(\"method\", \"get\").lower()\n",
    "\n",
    "                base_data = {inp[\"name\"]: (inp.get(\"value\") or \"test\")\n",
    "                             for inp in form[\"inputs\"] if inp.get(\"name\")}\n",
    "\n",
    "                for field in base_data:\n",
    "                    for payload in XSS_PAYLOADS:\n",
    "                        test_data = base_data.copy()\n",
    "                        test_data[field] = payload\n",
    "\n",
    "                        try:\n",
    "                            if method == \"post\":\n",
    "                                resp = self.session.post(action, data=test_data,\n",
    "                                                         timeout=self.timeout)\n",
    "                            else:\n",
    "                                resp = self.session.get(action, params=test_data,\n",
    "                                                        timeout=self.timeout)\n",
    "\n",
    "                            for pattern in REFLECT_PATTERNS:\n",
    "                                if pattern.search(resp.text):\n",
    "                                    self.findings.append({\n",
    "                                        \"type\": \"XSS-FORM\",\n",
    "                                        \"url\": action,\n",
    "                                        \"field\": field,\n",
    "                                        \"payload\": payload\n",
    "                                    })\n",
    "                                    print(f\"⚡ XSS detected in form {action} field={field} payload={payload}\")\n",
    "                                    break\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                        time.sleep(0.2)\n",
    "\n",
    "    def run(self, pages, forms):\n",
    "        \"\"\"Run scanner against crawler results.\"\"\"\n",
    "        for url in pages.keys():\n",
    "            self.test_url_params(url)\n",
    "        self.test_forms(forms)\n",
    "        return self.findings\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Main\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    base_url = \"http://localhost:8080\"  # Change to your DVWA URL\n",
    "\n",
    "    # Step 1: Crawl\n",
    "    crawler = DVWACrawler(base_url, username=\"admin\", password=\"password\",\n",
    "                          max_pages=10, delay=2)  # ⬅ safer delay\n",
    "    crawl_results = crawler.crawl()\n",
    "\n",
    "    # Step 2: SQL Injection Scan\n",
    "    sqli_scanner = SQLiScanner(crawl_results[\"session\"])\n",
    "    sqli_findings = sqli_scanner.run(crawl_results[\"pages\"], crawl_results[\"forms\"])\n",
    "\n",
    "    # Step 3: XSS Scan\n",
    "    xss_scanner = XSSScanner(crawl_results[\"session\"])\n",
    "    xss_findings = xss_scanner.run(crawl_results[\"pages\"], crawl_results[\"forms\"])\n",
    "\n",
    "    # Step 4: Pretty Report\n",
    "    print(\"\\n=== Vulnerability Findings ===\")\n",
    "    all_findings = sqli_findings + xss_findings\n",
    "    if all_findings:\n",
    "        for f in all_findings:\n",
    "            print(f\"[{f['type']}]\")\n",
    "            print(f\"   URL: {f['url']}\")\n",
    "            if 'parameter' in f:\n",
    "                print(f\"   Parameter: {f['parameter']}\")\n",
    "            if 'field' in f:\n",
    "                print(f\"   Field: {f['field']}\")\n",
    "            print(f\"   Payload: {f['payload']}\")\n",
    "            if 'error' in f:\n",
    "                print(f\"   Error: {f['error']}\")\n",
    "            print(\"-\" * 50)\n",
    "    else:\n",
    "        print(\"No SQLi or XSS vulnerabilities detected.\")\n",
    "\n",
    "    # Step 5: Export to CSV\n",
    "    with open(\"scan_results.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"type\", \"url\", \"parameter\", \"field\", \"payload\", \"error\"])\n",
    "        writer.writeheader()\n",
    "        for finding in all_findings:\n",
    "            writer.writerow(finding)\n",
    "    print(\"\\n📂 Results saved to scan_results.csv\")\n",
    "\n",
    "    print(\"\\n=== Mitigation Tips ===\")\n",
    "    print(\"SQLi Fixes: Use parameterized queries (PreparedStatements), ORM, input validation.\")\n",
    "    print(\"XSS Fixes: Validate input, encode output, apply Content Security Policy (CSP).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
